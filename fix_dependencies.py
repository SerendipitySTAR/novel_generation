#!/usr/bin/env python3
"""
ä¾èµ–ä¿®å¤è„šæœ¬
æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…
"""

import subprocess
import sys
import os
import importlib

def check_package(package_name, import_name=None):
    """æ£€æŸ¥åŒ…æ˜¯å¦å·²å®‰è£…"""
    if import_name is None:
        import_name = package_name
    
    try:
        importlib.import_module(import_name)
        return True
    except ImportError:
        return False

def install_package(package_name):
    """å®‰è£…åŒ…"""
    print(f"æ­£åœ¨å®‰è£… {package_name}...")
    try:
        # å°è¯•ä½¿ç”¨condaå®‰è£…ï¼ˆå¦‚æœåœ¨condaç¯å¢ƒä¸­ï¼‰
        if 'CONDA_DEFAULT_ENV' in os.environ:
            try:
                result = subprocess.run([
                    'conda', 'install', '-y', package_name
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    print(f"âœ… é€šè¿‡condaæˆåŠŸå®‰è£… {package_name}")
                    return True
                else:
                    print(f"âš ï¸  condaå®‰è£…å¤±è´¥ï¼Œå°è¯•pipå®‰è£…...")
            except (subprocess.TimeoutExpired, FileNotFoundError):
                print(f"âš ï¸  condaä¸å¯ç”¨ï¼Œå°è¯•pipå®‰è£…...")
        
        # ä½¿ç”¨pipå®‰è£…
        result = subprocess.run([
            sys.executable, '-m', 'pip', 'install', package_name
        ], capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print(f"âœ… é€šè¿‡pipæˆåŠŸå®‰è£… {package_name}")
            return True
        else:
            print(f"âŒ å®‰è£…å¤±è´¥: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"âŒ å®‰è£…è¶…æ—¶: {package_name}")
        return False
    except Exception as e:
        print(f"âŒ å®‰è£…å‡ºé”™: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–åŒ…...")
    
    # å®šä¹‰éœ€è¦æ£€æŸ¥çš„åŒ…
    required_packages = [
        ('sentence-transformers', 'sentence_transformers'),
        ('langchain', 'langchain'),
        ('langchain-community', 'langchain_community'),
        ('chromadb', 'chromadb'),
        ('openai', 'openai'),
        ('python-dotenv', 'dotenv'),
    ]
    
    missing_packages = []
    
    # æ£€æŸ¥æ¯ä¸ªåŒ…
    for package_name, import_name in required_packages:
        if check_package(package_name, import_name):
            print(f"âœ… {package_name} å·²å®‰è£…")
        else:
            print(f"âŒ {package_name} æœªå®‰è£…")
            missing_packages.append(package_name)
    
    if not missing_packages:
        print("\nğŸ‰ æ‰€æœ‰ä¾èµ–åŒ…éƒ½å·²å®‰è£…ï¼")
        return True
    
    print(f"\nğŸ“¦ å‘ç° {len(missing_packages)} ä¸ªç¼ºå¤±çš„åŒ…:")
    for pkg in missing_packages:
        print(f"  - {pkg}")
    
    # è¯¢é—®æ˜¯å¦å®‰è£…
    response = input("\næ˜¯å¦è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„åŒ…? (Y/n): ").strip().lower()
    if response == 'n':
        print("è·³è¿‡å®‰è£…ã€‚è¯·æ‰‹åŠ¨å®‰è£…ç¼ºå¤±çš„åŒ…ã€‚")
        return False
    
    # å®‰è£…ç¼ºå¤±çš„åŒ…
    print("\nğŸ”§ å¼€å§‹å®‰è£…ç¼ºå¤±çš„åŒ…...")
    success_count = 0
    
    for package_name in missing_packages:
        if install_package(package_name):
            success_count += 1
        else:
            print(f"âš ï¸  {package_name} å®‰è£…å¤±è´¥")
    
    print(f"\nğŸ“Š å®‰è£…ç»“æœ: {success_count}/{len(missing_packages)} ä¸ªåŒ…å®‰è£…æˆåŠŸ")
    
    if success_count == len(missing_packages):
        print("ğŸ‰ æ‰€æœ‰åŒ…éƒ½å®‰è£…æˆåŠŸï¼")
        
        # éªŒè¯å®‰è£…
        print("\nğŸ” éªŒè¯å®‰è£…...")
        all_ok = True
        for package_name, import_name in required_packages:
            if check_package(package_name, import_name):
                print(f"âœ… {package_name} éªŒè¯é€šè¿‡")
            else:
                print(f"âŒ {package_name} éªŒè¯å¤±è´¥")
                all_ok = False
        
        if all_ok:
            print("\nğŸ‰ æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…ï¼")
            return True
        else:
            print("\nâš ï¸  éƒ¨åˆ†ä¾èµ–éªŒè¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡å¯Pythonç¯å¢ƒ")
            return False
    else:
        print("\nâš ï¸  éƒ¨åˆ†åŒ…å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶æ‰‹åŠ¨å®‰è£…")
        return False

def test_knowledge_base():
    """æµ‹è¯•çŸ¥è¯†åº“åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•çŸ¥è¯†åº“åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import Chroma
        print("âœ… æˆåŠŸå¯¼å…¥çŸ¥è¯†åº“ç›¸å…³æ¨¡å—")
        
        # æµ‹è¯•æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„
        local_model_path = "/media/sc/AI/self-llm/embed_model/sentence-transformers/all-MiniLM-L6-v2"
        if os.path.exists(local_model_path):
            print(f"âœ… æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„å­˜åœ¨: {local_model_path}")
            
            # å°è¯•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            try:
                embeddings = HuggingFaceEmbeddings(
                    model_name=local_model_path,
                    model_kwargs={'device': 'cpu'}
                )
                print("âœ… æˆåŠŸåˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹")
                
                # æµ‹è¯•åµŒå…¥
                test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
                embedding = embeddings.embed_query(test_text)
                print(f"âœ… æˆåŠŸç”ŸæˆåµŒå…¥å‘é‡ (ç»´åº¦: {len(embedding)})")
                
                return True
                
            except Exception as e:
                print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                return False
        else:
            print(f"âš ï¸  æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_model_path}")
            print("   å°†å›é€€åˆ°OpenAIåµŒå…¥")
            return True
            
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def create_env_template():
    """åˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿"""
    env_template = """# å°è¯´ç”Ÿæˆç³»ç»Ÿç¯å¢ƒå˜é‡é…ç½®

# OpenAI APIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here

# æœ¬åœ°æ¨¡å‹é…ç½®
USE_LOCAL_EMBEDDINGS=true
LOCAL_EMBEDDING_MODEL_PATH=/media/sc/AI/self-llm/embed_model/sentence-transformers/all-MiniLM-L6-v2

# æ•°æ®åº“é…ç½®
DATABASE_NAME=novel_mvp.db
CHROMA_DB_DIRECTORY=./chroma_db

# å…¶ä»–é…ç½®
DEBUG=false
"""
    
    if not os.path.exists('.env'):
        print("\nğŸ“ åˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿æ–‡ä»¶...")
        with open('.env.template', 'w', encoding='utf-8') as f:
            f.write(env_template)
        print("âœ… å·²åˆ›å»º .env.template æ–‡ä»¶")
        print("   è¯·å¤åˆ¶ä¸º .env å¹¶å¡«å…¥æ­£ç¡®çš„é…ç½®å€¼")
    else:
        print("âœ… .env æ–‡ä»¶å·²å­˜åœ¨")

if __name__ == "__main__":
    print("ğŸ”§ å°è¯´ç”Ÿæˆç³»ç»Ÿä¾èµ–ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # åˆ‡æ¢åˆ°æ­£ç¡®ç›®å½•
    os.chdir('/media/sc/data/sc/novel_generation')
    
    # æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
    deps_ok = main()
    
    if deps_ok:
        # æµ‹è¯•çŸ¥è¯†åº“åŠŸèƒ½
        kb_ok = test_knowledge_base()
        
        if kb_ok:
            print("\nğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("\nâš ï¸  çŸ¥è¯†åº“åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œä½†åŸºæœ¬ä¾èµ–å·²å®‰è£…")
    
    # åˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿
    create_env_template()
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    if deps_ok:
        print("  1. âœ… ä¾èµ–å·²å®‰è£…ï¼Œå¯ä»¥è¿è¡Œç³»ç»Ÿ")
        print("  2. ğŸ”§ å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·é‡å¯Pythonç¯å¢ƒ")
        print("  3. ğŸ“ æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
    else:
        print("  1. âŒ è¯·æ‰‹åŠ¨å®‰è£…å¤±è´¥çš„ä¾èµ–åŒ…")
        print("  2. ğŸ” æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒåŒ…ç®¡ç†å™¨é…ç½®")
        print("  3. ğŸ“ å¦‚éœ€å¸®åŠ©ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯")
    
    print("\næµ‹è¯•å‘½ä»¤:")
    print("  python3 test_problem_solver.py")
    print("  python3 cleanup_memory_issues.py")
